<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Filter Web App</title>
    <style>
        #container {
            position: relative;
        }
        #video, #canvas, #filter {
            position: absolute;
            top: 0;
            left: 0;
        }
        #filter {
            display: none;
        }
        #buttons {
            position: absolute;
            top: 10px;
            left: 10px;
            z-index: 1;
        }
    </style>
</head>
<body>
    <h1>Face Filter Web App</h1>
    
    <div id="container">
        <video id="video" width="640" height="480" autoplay></video>
        <canvas id="canvas" width="640" height="480"></canvas>
        <img id="filter" src="Cow.png" alt="Cow Filter">
        <div id="buttons">
            <button id="applyFilter">Apply Filter</button>
            <button id="clearFilter">Clear Filter</button>
        </div>
    </div>

    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const context = canvas.getContext('2d');
        const filterImage = document.getElementById('filter');
        const videoWidth = 640;
        const videoHeight = 480;

        async function setupCamera() {
            const stream = await navigator.mediaDevices.getUserMedia({ video: true });
            video.srcObject = stream;
        }

        async function loadFaceAPI() {
            await faceapi.nets.tinyFaceDetector.loadFromUri('/models');
            await faceapi.nets.faceLandmark68Net.loadFromUri('/models');
            await faceapi.nets.faceRecognitionNet.loadFromUri('/models');
        }

        async function startFaceTracking() {
            await setupCamera();
            await loadFaceAPI();
            trackFace();
        }

        async function trackFace() {
            const faces = await faceapi.detectSingleFace(video).withFaceLandmarks();

            if (faces) {
                const face = faces.detection.box;

                const filterWidth = face.width * 1.5;
                const filterHeight = face.height * 1.5;
                const filterX = face.x - (filterWidth - face.width) / 2;
                const filterY = face.y - (filterHeight - face.height) / 2;

                context.clearRect(0, 0, canvas.width, canvas.height);
                context.drawImage(video, 0, 0, canvas.width, canvas.height);
                context.drawImage(filterImage, filterX, filterY, filterWidth, filterHeight);
            }

            requestAnimationFrame(trackFace);
        }

        document.getElementById('applyFilter').addEventListener('click', startFaceTracking);

        function clearFilter() {
            context.clearRect(0, 0, canvas.width, canvas.height);
        }

        document.getElementById('clearFilter').addEventListener('click', clearFilter);

        // Load the Face API model when the page loads
        startFaceTracking();
    </script>
    
    <!-- Load Face API -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
</body>
</html>
